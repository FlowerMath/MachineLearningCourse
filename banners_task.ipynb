{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что международное круизное агентство Carnival Cruise Line решило себя разрекламировать с помощью баннеров и обратилось для этого к вам. Чтобы протестировать, велика ли от таких баннеров польза, их будет размещено всего 20 штук по всему миру. Вам надо выбрать 20 таких локаций для размещения, чтобы польза была большой и агентство продолжило с вами сотрудничать.\n",
    "\n",
    "Агентство крупное, и у него есть несколько офисов по всему миру. Вблизи этих офисов оно и хочет разместить баннеры — легче договариваться и проверять результат. Также эти места должны быть популярны среди туристов.\n",
    "\n",
    "Для поиска оптимальных мест воспользуемся базой данных крупнейшей социальной сети, основанной на локациях — Foursquare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью pandas построим DataFrame и убедимся, что все 396634 строки с координатами считаны успешно.\n",
    "\n",
    "Теперь необходимо кластеризовать данные координаты, чтобы выявить центры скоплений туристов. Поскольку баннеры имеют сравнительно небольшую площадь действия, нам нужен алгоритм, позволяющий ограничить размер кластера и не зависящий от количества кластеров.\n",
    "\n",
    "Эта задача — хороший повод познакомиться с алгоритмом MeanShift, который мы обошли стороной в основной части лекций. Его описание при желании можно посмотреть в sklearn user guide, а чуть позже появится дополнительное видео с обзором этого и некоторых других алгоритмов кластеризации. Используйте MeanShift, указав bandwidth=0.1, что в переводе из градусов в метры колеблется примерно от 5 до 10 км в средних широтах.\n",
    "\n",
    "Примечание: на 396634 строках кластеризация будет работать долго. Быть очень терпеливым не возбраняется — результат от этого только улучшится. Но для того, чтобы сдать задание, понадобится сабсет из первых 100 тысяч строк. Это компромисс между качеством и затраченным временем. Обучение алгоритма на всём датасете занимает около часа, а на 100 тыс. строк — примерно 2 минуты, однако этого достаточно для получения корректных результатов.\n",
    "\n",
    "Некоторые из получившихся кластеров содержат слишком мало точек — такие кластеры не интересны рекламодателям. Поэтому надо определить, какие из кластеров содержат, скажем, больше 15 элементов. Центры этих кластеров и являются оптимальными для размещения.\n",
    "\n",
    "При желании увидеть получившиеся результаты на карте можно передать центры получившихся кластеров в один из инструментов визуализации. Например, сайт mapcustomizer.com имеет функцию Bulk Entry, куда можно вставить центры полученных кластеров в формате:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определить 20 ближайших к ним центров кластеров. Т.е. посчитать дистанцию до ближайшего офиса для каждой точки и выбрать 20 с наименьшим значением.\n",
    "\n",
    "Примечание: при подсчете расстояний и в кластеризации можно пренебречь тем, что Земля круглая, так как в точках, расположенных близко друг к другу погрешность мала, а в остальных точках значение достаточно велико.\n",
    "\n",
    "Для сдачи задания выберите из получившихся 20 центров тот, который наименее удален от ближайшего к нему офиса. Ответ в этом задании — широта и долгота этого центра, записанные через пробел.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkins.dat', 'r') as file1:\n",
    "    with open('checkins-new.dat', 'w') as file2:\n",
    "        for index, line in enumerate(file1):\n",
    "            newLine = line.replace(' ','').replace('|', ',')\n",
    "            if (index != 1):\n",
    "                file2.write(newLine)\n",
    "                \n",
    "data = pd.read_csv('checkins-new.dat').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(data)\n",
    "cols = [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:100000,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=0.1, bin_seeding=False, cluster_all=True, min_bin_freq=1,\n",
       "     n_jobs=1, seeds=None)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = MeanShift(bandwidth=0.1)\n",
    "cluster.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts1 = np.unique(cluster.labels_, return_counts=True)\n",
    "label_counts1 = zip(label_counts1[0], label_counts1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_label_count = sorted(label_counts1, key = lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [center for center, count in sorted_label_count if count > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = [(center, count) for center, count in sorted_label_count if count > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_centers = centers[array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "offices = [[33.751277,-118.188740],\n",
    "[25.867736,-80.324116],\n",
    "[51.503016,-0.075479],\n",
    "[52.378894,4.885084],\n",
    "[39.366487,117.036146],\n",
    "[-33.868457,151.205134]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofnorm = []\n",
    "for x in array_centers:\n",
    "    m = np.linalg.norm(x - offices[0])\n",
    "    for i in range(1, len(offices)):\n",
    "        if np.linalg.norm(x - offices[i]) < m:\n",
    "            m = np.linalg.norm(x - offices[i])\n",
    "    listofnorm.append((x, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofnorm_sort = sorted(listofnorm, key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([-33.86063043, 151.20477593]), 0.007834758163107856),\n",
       " (array([52.37296399,  4.89231722]), 0.009353316185992226),\n",
       " (array([ 25.84567226, -80.3188906 ]), 0.022674066158385495),\n",
       " (array([51.50299126, -0.12553729]), 0.05005829482278787),\n",
       " (array([  33.80987796, -118.14892381]), 0.07084773242719973),\n",
       " (array([ 25.78581242, -80.21793804]), 0.13410903336184654),\n",
       " (array([ 25.70534972, -80.28342874]), 0.16740596425035326),\n",
       " (array([ 26.01009825, -80.19999059]), 0.18887596060185083),\n",
       " (array([  33.88832534, -118.04892817]), 0.19577945647763628),\n",
       " (array([  33.87298601, -118.36209115]), 0.21181053682436798),\n",
       " (array([  33.97257482, -118.16837067]), 0.2222332907317907),\n",
       " (array([ 26.13884379, -80.33434684]), 0.2713007595066735),\n",
       " (array([  33.98393587, -118.00740497]), 0.2949788868004569),\n",
       " (array([ 26.12086266, -80.15890668]), 0.3022701186924605),\n",
       " (array([  33.81730643, -117.89124917]), 0.30473050307840693),\n",
       " (array([  34.06039755, -118.24870903]), 0.3148837903362732),\n",
       " (array([  33.67430266, -117.85878927]), 0.3388104702511318),\n",
       " (array([ 26.20058464, -80.25071613]), 0.3408456533220572),\n",
       " (array([  34.03548695, -118.43899772]), 0.37868750125029754),\n",
       " (array([  34.13146015, -118.11801181]), 0.3867062248427277)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listofnorm_sort[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer(answer,name):\n",
    "    with open(name, \"w\") as fout:\n",
    "        fout.write(\" \".join([str(num) for num in answer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(listofnorm_sort[0][0],'banners.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
